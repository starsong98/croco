{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bca0f41",
   "metadata": {},
   "source": [
    "# Simple inference example with CroCo-Stereo or CroCo-Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80653ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (C) 2022-present Naver Corporation. All rights reserved.\n",
    "# Licensed under CC BY-NC-SA 4.0 (non-commercial use only)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f033862",
   "metadata": {},
   "source": [
    "First download the model(s) of your choice by running\n",
    "```\n",
    "bash stereoflow/download_model.sh crocostereo.pth\n",
    "bash stereoflow/download_model.sh crocoflow.pth\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb2e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "use_gpu = torch.cuda.is_available() and torch.cuda.device_count()>0\n",
    "device = torch.device('cuda:0' if use_gpu else 'cpu')\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e25d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stereoflow.test import _load_model_and_criterion\n",
    "from stereoflow.engine import tiled_pred\n",
    "from stereoflow.datasets_stereo import img_to_tensor, vis_disparity\n",
    "from stereoflow.datasets_flow import flowToColor\n",
    "tile_overlap=0.7 # recommended value, higher value can be slightly better but slower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a921f5",
   "metadata": {},
   "source": [
    "### CroCo-Stereo example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64e483cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = np.asarray(Image.open('datasets_realvideo/DAVIS_SAMPLE/frame_0000.png'))\n",
    "image2 = np.asarray(Image.open('datasets_realvideo/DAVIS_SAMPLE/frame_0001.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d04303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model, _, cropsize, with_conf, task, tile_conf_mode = _load_model_and_criterion('stereoflow_models/crocostereo.pth', None, device)\n",
    "#model, _, cropsize, with_conf, task, tile_conf_mode = _load_model_and_criterion('stereoflow_models/crocoflow.pth', None, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dc14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = img_to_tensor(image1).to(device).unsqueeze(0)\n",
    "im2 = img_to_tensor(image2).to(device).unsqueeze(0)\n",
    "with torch.inference_mode():\n",
    "    pred, _, _ = tiled_pred(model, None, im1, im2, None, conf_mode=tile_conf_mode, overlap=tile_overlap, crop=cropsize, with_conf=with_conf, return_time=False)\n",
    "pred = pred.squeeze(0).squeeze(0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583b9f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(vis_disparity(pred))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df5d70",
   "metadata": {},
   "source": [
    "### CroCo-Flow example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ee257a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image1 = np.asarray(Image.open('<path_to_first_image>'))\n",
    "#image2 = np.asarray(Image.open('<path_to_second_image>'))\n",
    "#image1 = np.asarray(Image.open('datasets_realvideo/DAVIS_SAMPLE/frame_0000.png'))\n",
    "#image2 = np.asarray(Image.open('datasets_realvideo/DAVIS_SAMPLE/frame_0001.png'))\n",
    "image1 = np.asarray(Image.open('datasets_realvideo/SINTEL_SAMPLE/frame_0015.png'))\n",
    "image2 = np.asarray(Image.open('datasets_realvideo/SINTEL_SAMPLE/frame_0016.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5edccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _, cropsize, with_conf, task, tile_conf_mode = _load_model_and_criterion('stereoflow_models/crocoflow.pth', None, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b19692c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = img_to_tensor(image1).to(device).unsqueeze(0)\n",
    "im2 = img_to_tensor(image2).to(device).unsqueeze(0)\n",
    "with torch.inference_mode():\n",
    "    pred, _, _ = tiled_pred(model, None, im1, im2, None, conf_mode=tile_conf_mode, overlap=tile_overlap, crop=cropsize, with_conf=with_conf, return_time=False)\n",
    "pred = pred.squeeze(0).permute(1,2,0).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f79db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(flowToColor(pred))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c553311",
   "metadata": {},
   "source": [
    "### CroCo-Flow sequence inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "000a08d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning, cannot find cuda-compiled version of RoPE2D, using a slow pytorch version instead\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "use_gpu = torch.cuda.is_available() and torch.cuda.device_count()>0\n",
    "device = torch.device('cuda:0' if use_gpu else 'cpu')\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "\n",
    "from stereoflow.test import _load_model_and_criterion\n",
    "from stereoflow.engine import tiled_pred, tiled_pred_tqdm\n",
    "from stereoflow.datasets_stereo import img_to_tensor, vis_disparity\n",
    "from stereoflow.datasets_flow import flowToColor\n",
    "from stereoflow.warp_utils import fwarp_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "312e2c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_overlap=0.7 # recommended value, higher value can be slightly better but slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebdb32f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model from stereoflow_models/crocoflow.pth\n",
      "head: PixelwiseTaskWithDPT()\n",
      "croco_args: {'enc_embed_dim': 1024, 'enc_depth': 24, 'enc_num_heads': 16, 'dec_embed_dim': 768, 'dec_num_heads': 12, 'dec_depth': 12, 'img_size': (320, 384), 'pos_embed': 'RoPE100'}\n",
      "  PixelwiseTaskWithDPT: automatically setting hook_idxs=[23, 27, 31, 35]\n"
     ]
    }
   ],
   "source": [
    "model, _, cropsize, with_conf, task, tile_conf_mode = _load_model_and_criterion('stereoflow_models/crocoflow.pth', None, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546ff1a2",
   "metadata": {},
   "source": [
    "#### Inference on Single video clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f1816bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117a86cc284b4a14ba05791dc9c72277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaist/Documents/OFE/croco/stereoflow/datasets_stereo.py:45: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  img = torch.from_numpy(img).permute(2, 0, 1).float() / 255.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb318b39d3d544a4953ed39fd4cf52b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tiles:   0%|          | 0/714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m im2 \u001b[39m=\u001b[39m img_to_tensor(image2)\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[1;32m     45\u001b[0m     \u001b[39m#pred, _, _ = tiled_pred(model, None, im1, im2, None, conf_mode=tile_conf_mode, overlap=tile_overlap, crop=cropsize, with_conf=with_conf, return_time=False)\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     pred, _, _ \u001b[39m=\u001b[39m tiled_pred_tqdm(model, \u001b[39mNone\u001b[39;49;00m, im1, im2, \u001b[39mNone\u001b[39;49;00m, conf_mode\u001b[39m=\u001b[39;49mtile_conf_mode, overlap\u001b[39m=\u001b[39;49mtile_overlap, crop\u001b[39m=\u001b[39;49mcropsize, with_conf\u001b[39m=\u001b[39;49mwith_conf, return_time\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     47\u001b[0m flo \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mcpu()  \u001b[39m# to be used for warping ops\u001b[39;00m\n\u001b[1;32m     48\u001b[0m pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/OFE/croco/stereoflow/engine.py:326\u001b[0m, in \u001b[0;36mtiled_pred_tqdm\u001b[0;34m(model, criterion, img1, img2, gt, overlap, bad_crop_thr, downscale, crop, ret, conf_mode, with_conf, return_time)\u001b[0m\n\u001b[1;32m    322\u001b[0m     start\u001b[39m.\u001b[39mrecord()\n\u001b[1;32m    324\u001b[0m \u001b[39mfor\u001b[39;00m sy1, sx1, sy2, sx2, aligned \u001b[39min\u001b[39;00m tqdm(crop_generator(), total\u001b[39m=\u001b[39mtiles_count, desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtiles\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    325\u001b[0m     \u001b[39m# compute optical flow there\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m     pred \u001b[39m=\u001b[39m  model(_crop(img1,sy1,sx1), _crop(img2,sy2,sx2))\n\u001b[1;32m    327\u001b[0m     pred, predconf \u001b[39m=\u001b[39m split_prediction_conf(pred, with_conf\u001b[39m=\u001b[39mwith_conf)\n\u001b[1;32m    329\u001b[0m     \u001b[39mif\u001b[39;00m gt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: gtcrop \u001b[39m=\u001b[39m _crop(gt,sy1,sx1)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/OFE/croco/models/croco_downstream.py:116\u001b[0m, in \u001b[0;36mCroCoDownstreamBinocular.forward\u001b[0;34m(self, img1, img2)\u001b[0m\n\u001b[1;32m    114\u001b[0m img_info \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mheight\u001b[39m\u001b[39m'\u001b[39m: H, \u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m: W}\n\u001b[1;32m    115\u001b[0m return_all_blocks \u001b[39m=\u001b[39m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead, \u001b[39m'\u001b[39m\u001b[39mreturn_all_blocks\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead\u001b[39m.\u001b[39mreturn_all_blocks\n\u001b[0;32m--> 116\u001b[0m out, out2, pos, pos2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_image_pairs(img1, img2, return_all_blocks\u001b[39m=\u001b[39;49mreturn_all_blocks)\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m return_all_blocks:\n\u001b[1;32m    118\u001b[0m     decout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoder(out[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], pos, \u001b[39mNone\u001b[39;00m, out2, pos2, return_all_blocks\u001b[39m=\u001b[39mreturn_all_blocks)\n",
      "File \u001b[0;32m~/Documents/OFE/croco/models/croco_downstream.py:103\u001b[0m, in \u001b[0;36mCroCoDownstreamBinocular.encode_image_pairs\u001b[0;34m(self, img1, img2, return_all_blocks)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" run encoder for a pair of images\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39m    it is actually ~5% faster to concatenate the images along the batch dimension \u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39m     than to encode them separately\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m## the two commented lines below is the naive version with separate encoding\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39m#out, pos, _ = self._encode_image(img1, do_mask=False, return_all_blocks=return_all_blocks)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39m#out2, pos2, _ = self._encode_image(img2, do_mask=False, return_all_blocks=False)\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[39m## and now the faster version\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m out, pos, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_image( torch\u001b[39m.\u001b[39;49mcat( (img1,img2), dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), do_mask\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, return_all_blocks\u001b[39m=\u001b[39;49mreturn_all_blocks )\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m return_all_blocks:\n\u001b[1;32m    105\u001b[0m     out,out2 \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlist\u001b[39m, \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[o\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m out])))\n",
      "File \u001b[0;32m~/Documents/OFE/croco/models/croco.py:156\u001b[0m, in \u001b[0;36mCroCoNet._encode_image\u001b[0;34m(self, image, do_mask, return_all_blocks)\u001b[0m\n\u001b[1;32m    154\u001b[0m out \u001b[39m=\u001b[39m []\n\u001b[1;32m    155\u001b[0m \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc_blocks:\n\u001b[0;32m--> 156\u001b[0m     x \u001b[39m=\u001b[39m blk(x, posvis)\n\u001b[1;32m    157\u001b[0m     out\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m    158\u001b[0m out[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc_norm(out[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/OFE/croco/models/blocks.py:128\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, xpos)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, xpos):\n\u001b[0;32m--> 128\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_path(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(x), xpos))\n\u001b[1;32m    129\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x)))\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/OFE/croco/models/blocks.py:102\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x, xpos)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39m# q,k,v = qkv.unbind(2)  # make torchscript happy (cannot use tensor as tuple)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrope \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrope(q, xpos)\n\u001b[1;32m    103\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrope(k, xpos)\n\u001b[1;32m    105\u001b[0m attn \u001b[39m=\u001b[39m (q \u001b[39m@\u001b[39m k\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/OFE/croco/models/pos_embed.py:151\u001b[0m, in \u001b[0;36mRoPE2D.forward\u001b[0;34m(self, tokens, positions)\u001b[0m\n\u001b[1;32m    149\u001b[0m D \u001b[39m=\u001b[39m tokens\u001b[39m.\u001b[39msize(\u001b[39m3\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    150\u001b[0m \u001b[39massert\u001b[39;00m positions\u001b[39m.\u001b[39mndim\u001b[39m==\u001b[39m\u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m positions\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39m# Batch, Seq, 2\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m cos, sin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_cos_sin(D, \u001b[39mint\u001b[39;49m(positions\u001b[39m.\u001b[39;49mmax())\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, tokens\u001b[39m.\u001b[39mdevice, tokens\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    152\u001b[0m \u001b[39m# split features into two along the feature dimension, and apply rope1d on each half\u001b[39;00m\n\u001b[1;32m    153\u001b[0m y, x \u001b[39m=\u001b[39m tokens\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load video sequence & handle saving directories\n",
    "#sequence_root = \"datasets_realvideo/DAVIS_SAMPLE\"\n",
    "#sequence_root = \"datasets_realvideo/SINTEL_SAMPLE\"\n",
    "sequence_root = \"datasets_realvideo/XVFI/Longer_testset/Type1/TEST01_003_f0433\"\n",
    "#save_root = \"stereoflow_models/crocoflow.pth_debugs\"\n",
    "save_root = \"stereoflow_models/crocoflow.pth_XVFI\"\n",
    "\n",
    "image_list_raw = sorted(os.listdir(sequence_root))\n",
    "image_list = []\n",
    "for i in range(len(image_list_raw)-1):\n",
    "    image_list += [\n",
    "        [\n",
    "            os.path.join(sequence_root, image_list_raw[i]),\n",
    "            os.path.join(sequence_root, image_list_raw[i+1])\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "save_dir = os.path.join(save_root, os.path.basename(sequence_root))\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_dir_vis = os.path.join(save_root, os.path.basename(sequence_root), 'flow_vis')\n",
    "os.makedirs(save_dir_vis, exist_ok=True)\n",
    "save_dir_fwarp = os.path.join(save_root, os.path.basename(sequence_root), 'image1_fwarp')\n",
    "os.makedirs(save_dir_fwarp, exist_ok=True)\n",
    "save_dir_bwarp = os.path.join(save_root, os.path.basename(sequence_root), 'image1_bwarp')\n",
    "os.makedirs(save_dir_bwarp, exist_ok=True)\n",
    "save_dir_grid = os.path.join(save_root, os.path.basename(sequence_root), 'grid_vis')\n",
    "os.makedirs(save_dir_grid, exist_ok=True)\n",
    "\n",
    "print()\n",
    "\n",
    "# actual execution\n",
    "for i in tqdm(range(len(image_list))):\n",
    "    image1_name = image_list[i][0]\n",
    "    image2_name = image_list[i][1]\n",
    "\n",
    "    image1 = np.asarray(Image.open(image1_name))\n",
    "    image2 = np.asarray(Image.open(image2_name))\n",
    "    #image1 = Image.open(image1_name).convert('RGB')\n",
    "    #image2 = Image.open(image2_name).convert('RGB')\n",
    "    #image1 = np.array(image1).astype(np.uint8)[..., :3]\n",
    "    #image2 = np.array(image2).astype(np.uint8)[..., :3]\n",
    "    im1 = img_to_tensor(image1).to(device).unsqueeze(0)\n",
    "    im2 = img_to_tensor(image2).to(device).unsqueeze(0)\n",
    "    with torch.inference_mode():\n",
    "        #pred, _, _ = tiled_pred(model, None, im1, im2, None, conf_mode=tile_conf_mode, overlap=tile_overlap, crop=cropsize, with_conf=with_conf, return_time=False)\n",
    "        pred, _, _ = tiled_pred_tqdm(model, None, im1, im2, None, conf_mode=tile_conf_mode, overlap=tile_overlap, crop=cropsize, with_conf=with_conf, return_time=False)\n",
    "    flo = pred.clone().cpu()  # to be used for warping ops\n",
    "    pred = pred.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "    # visualize flow\n",
    "    flow_vis = flowToColor(pred)\n",
    "    flow_vis_path = os.path.join(save_dir_vis, os.path.basename(image1_name))\n",
    "    cv2.imwrite(flow_vis_path, flow_vis[:, :, [2,1,0]])\n",
    "\n",
    "    # visualize forward warping\n",
    "    img = torch.from_numpy(image1).permute(2, 0, 1).unsqueeze(0)\n",
    "    image1_fwarp, _ = fwarp_wrapper(img=img, flo=flo)\n",
    "    fwarp_path = os.path.join(save_dir_fwarp, os.path.basename(image1_name))\n",
    "    cv2.imwrite(fwarp_path, image1_fwarp[:, :, [2,1,0]])\n",
    "\n",
    "    # grid visualization\n",
    "    row_1 = np.concatenate([image1, image2], axis=1)\n",
    "    row_2 = np.concatenate([flow_vis, image1_fwarp], axis=1)\n",
    "    grid_vis = np.concatenate([row_1, row_2], axis=0)\n",
    "    grid_vis_path = os.path.join(save_dir_grid, os.path.basename(image1_name))\n",
    "    cv2.imwrite(grid_vis_path, grid_vis[:, :, [2,1,0]])\n",
    "\n",
    "\n",
    "    #plt.imshow(image1_fwarp[:, :, [2,1,0]])\n",
    "    #plt.imshow(image1_fwarp)\n",
    "    #plt.axis('off')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b07da7d",
   "metadata": {},
   "source": [
    "#### Inference on multiple video clips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e479674c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70568d9f7b504e81bd7953668b8af25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing sequences:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ad307528fd4ce68280f038bbdde1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing frames in sequence TEST02_045_f0465:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2201c256cd174e2ab1cf06dc3d6e8e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tiles:   0%|          | 0/714 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m im2 \u001b[39m=\u001b[39m img_to_tensor(image2)\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[1;32m     65\u001b[0m     \u001b[39m#pred, _, _ = tiled_pred(model, None, im1, im2, None, conf_mode=tile_conf_mode, overlap=tile_overlap, crop=cropsize, with_conf=with_conf, return_time=False)\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     pred, _, _ \u001b[39m=\u001b[39m tiled_pred_tqdm(model, \u001b[39mNone\u001b[39;49;00m, im1, im2, \u001b[39mNone\u001b[39;49;00m, conf_mode\u001b[39m=\u001b[39;49mtile_conf_mode, overlap\u001b[39m=\u001b[39;49mtile_overlap, crop\u001b[39m=\u001b[39;49mcropsize, with_conf\u001b[39m=\u001b[39;49mwith_conf, return_time\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     67\u001b[0m flo \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mcpu()  \u001b[39m# to be used for warping ops\u001b[39;00m\n\u001b[1;32m     68\u001b[0m pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/OFE/croco/stereoflow/engine.py:326\u001b[0m, in \u001b[0;36mtiled_pred_tqdm\u001b[0;34m(model, criterion, img1, img2, gt, overlap, bad_crop_thr, downscale, crop, ret, conf_mode, with_conf, return_time)\u001b[0m\n\u001b[1;32m    322\u001b[0m     start\u001b[39m.\u001b[39mrecord()\n\u001b[1;32m    324\u001b[0m \u001b[39mfor\u001b[39;00m sy1, sx1, sy2, sx2, aligned \u001b[39min\u001b[39;00m tqdm(crop_generator(), total\u001b[39m=\u001b[39mtiles_count, desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtiles\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    325\u001b[0m     \u001b[39m# compute optical flow there\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m     pred \u001b[39m=\u001b[39m  model(_crop(img1,sy1,sx1), _crop(img2,sy2,sx2))\n\u001b[1;32m    327\u001b[0m     pred, predconf \u001b[39m=\u001b[39m split_prediction_conf(pred, with_conf\u001b[39m=\u001b[39mwith_conf)\n\u001b[1;32m    329\u001b[0m     \u001b[39mif\u001b[39;00m gt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: gtcrop \u001b[39m=\u001b[39m _crop(gt,sy1,sx1)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/OFE/croco/models/croco_downstream.py:118\u001b[0m, in \u001b[0;36mCroCoDownstreamBinocular.forward\u001b[0;34m(self, img1, img2)\u001b[0m\n\u001b[1;32m    116\u001b[0m out, out2, pos, pos2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_image_pairs(img1, img2, return_all_blocks\u001b[39m=\u001b[39mreturn_all_blocks)\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m return_all_blocks:\n\u001b[0;32m--> 118\u001b[0m     decout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decoder(out[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], pos, \u001b[39mNone\u001b[39;49;00m, out2, pos2, return_all_blocks\u001b[39m=\u001b[39;49mreturn_all_blocks)\n\u001b[1;32m    119\u001b[0m     decout \u001b[39m=\u001b[39m out\u001b[39m+\u001b[39mdecout\n\u001b[1;32m    120\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/OFE/croco/models/croco.py:194\u001b[0m, in \u001b[0;36mCroCoNet._decoder\u001b[0;34m(self, feat1, pos1, masks1, feat2, pos2, return_all_blocks)\u001b[0m\n\u001b[1;32m    192\u001b[0m _out, out \u001b[39m=\u001b[39m out, []\n\u001b[1;32m    193\u001b[0m \u001b[39mfor\u001b[39;00m blk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdec_blocks:\n\u001b[0;32m--> 194\u001b[0m     _out, out2 \u001b[39m=\u001b[39m blk(_out, out2, pos1, pos2)\n\u001b[1;32m    195\u001b[0m     out\u001b[39m.\u001b[39mappend(_out)\n\u001b[1;32m    196\u001b[0m out[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdec_norm(out[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/OFE/croco/models/blocks.py:187\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[0;34m(self, x, y, xpos, ypos)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, y, xpos, ypos):\n\u001b[0;32m--> 187\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_path(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(x), xpos))\n\u001b[1;32m    188\u001b[0m     y_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_y(y)\n\u001b[1;32m    189\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcross_attn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x), y_, y_, xpos, ypos))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/OFE/croco/models/blocks.py:102\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x, xpos)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[39m# q,k,v = qkv.unbind(2)  # make torchscript happy (cannot use tensor as tuple)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrope \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrope(q, xpos)\n\u001b[1;32m    103\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrope(k, xpos)\n\u001b[1;32m    105\u001b[0m attn \u001b[39m=\u001b[39m (q \u001b[39m@\u001b[39m k\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/OFE/croco/models/pos_embed.py:151\u001b[0m, in \u001b[0;36mRoPE2D.forward\u001b[0;34m(self, tokens, positions)\u001b[0m\n\u001b[1;32m    149\u001b[0m D \u001b[39m=\u001b[39m tokens\u001b[39m.\u001b[39msize(\u001b[39m3\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    150\u001b[0m \u001b[39massert\u001b[39;00m positions\u001b[39m.\u001b[39mndim\u001b[39m==\u001b[39m\u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m positions\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39m# Batch, Seq, 2\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m cos, sin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_cos_sin(D, \u001b[39mint\u001b[39;49m(positions\u001b[39m.\u001b[39;49mmax())\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, tokens\u001b[39m.\u001b[39mdevice, tokens\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    152\u001b[0m \u001b[39m# split features into two along the feature dimension, and apply rope1d on each half\u001b[39;00m\n\u001b[1;32m    153\u001b[0m y, x \u001b[39m=\u001b[39m tokens\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# load MULTIPLE video sequence & handle saving directories\n",
    "#sequence_root = \"datasets_realvideo/DAVIS_SAMPLE\"\n",
    "#sequence_root = \"datasets_realvideo/SINTEL_SAMPLE\"\n",
    "#sequence_root = \"datasets_realvideo/XVFI/Longer_testset/Type1/TEST01_003_f0433\"\n",
    "sequence_roots = [\n",
    "    #\"datasets_realvideo/SINTEL_SAMPLE/\",\n",
    "    #\"datasets_realvideo/DAVIS_SAMPLE/\",\n",
    "    #\"datasets_realvideo/XVFI/Longer_testset/Type1/TEST01_003_f0433\",\n",
    "    \"datasets_realvideo/XVFI/Longer_testset/Type1/TEST02_045_f0465\",\n",
    "    \"datasets_realvideo/XVFI/Longer_testset/Type1/TEST03_081_f4833\",\n",
    "    \"datasets_realvideo/XVFI/Longer_testset/Type1/TEST04_140_f3889\",\n",
    "    \"datasets_realvideo/XVFI/Longer_testset/Type1/TEST05_158_f0321\",\n",
    "    \"datasets_realvideo/XVFI/Longer_testset/Type2/TEST06_001_f0273\",\n",
    "    \"datasets_realvideo/XVFI/Longer_testset/Type2/TEST07_076_f1889\",\n",
    "    \"datasets_realvideo/XVFI/Longer_testset/Type2/TEST08_079_f0321\",\n",
    "    \"datasets_realvideo/XVFI/Longer_testset/Type2/TEST09_112_f0177\",\n",
    "    \"datasets_realvideo/XVFI/Longer_testset/Type2/TEST10_172_f1905\",\n",
    "    \"datasets_realvideo/XVFI/Longer_testset/Type3/TEST11_078_f4977\",\n",
    "    \"datasets_realvideo/XVFI/Longer_testset/Type3/TEST12_087_f2721\",\n",
    "    \"datasets_realvideo/XVFI/Longer_testset/Type3/TEST13_133_f4593\",\n",
    "    \"datasets_realvideo/XVFI/Longer_testset/Type3/TEST14_146_f1761\",\n",
    "    \"datasets_realvideo/XVFI/Longer_testset/Type3/TEST15_148_f0465\",\n",
    "]\n",
    "#save_root = \"stereoflow_models/crocoflow.pth_debugs\"\n",
    "save_root = \"stereoflow_models/crocoflow.pth_XVFI\"\n",
    "\n",
    "for sequence_root in tqdm(sequence_roots, desc='processing sequences'):\n",
    "    image_list_raw = sorted(os.listdir(sequence_root))\n",
    "    image_list = []\n",
    "    for i in range(len(image_list_raw)-1):\n",
    "        image_list += [\n",
    "            [\n",
    "                os.path.join(sequence_root, image_list_raw[i]),\n",
    "                os.path.join(sequence_root, image_list_raw[i+1])\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "    save_dir = os.path.join(save_root, os.path.basename(sequence_root))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_dir_vis = os.path.join(save_root, os.path.basename(sequence_root), 'flow_vis')\n",
    "    os.makedirs(save_dir_vis, exist_ok=True)\n",
    "    save_dir_fwarp = os.path.join(save_root, os.path.basename(sequence_root), 'image1_fwarp')\n",
    "    os.makedirs(save_dir_fwarp, exist_ok=True)\n",
    "    save_dir_bwarp = os.path.join(save_root, os.path.basename(sequence_root), 'image1_bwarp')\n",
    "    os.makedirs(save_dir_bwarp, exist_ok=True)\n",
    "    save_dir_grid = os.path.join(save_root, os.path.basename(sequence_root), 'grid_vis')\n",
    "    os.makedirs(save_dir_grid, exist_ok=True)\n",
    "\n",
    "    print()\n",
    "\n",
    "    # actual execution\n",
    "    for i in tqdm(range(len(image_list)), desc=f'processing frames in sequence {os.path.basename(sequence_root)}'):\n",
    "        image1_name = image_list[i][0]\n",
    "        image2_name = image_list[i][1]\n",
    "\n",
    "        image1 = np.asarray(Image.open(image1_name))\n",
    "        image2 = np.asarray(Image.open(image2_name))\n",
    "        #image1 = Image.open(image1_name).convert('RGB')\n",
    "        #image2 = Image.open(image2_name).convert('RGB')\n",
    "        #image1 = np.array(image1).astype(np.uint8)[..., :3]\n",
    "        #image2 = np.array(image2).astype(np.uint8)[..., :3]\n",
    "        im1 = img_to_tensor(image1).to(device).unsqueeze(0)\n",
    "        im2 = img_to_tensor(image2).to(device).unsqueeze(0)\n",
    "        with torch.inference_mode():\n",
    "            #pred, _, _ = tiled_pred(model, None, im1, im2, None, conf_mode=tile_conf_mode, overlap=tile_overlap, crop=cropsize, with_conf=with_conf, return_time=False)\n",
    "            pred, _, _ = tiled_pred_tqdm(model, None, im1, im2, None, conf_mode=tile_conf_mode, overlap=tile_overlap, crop=cropsize, with_conf=with_conf, return_time=False)\n",
    "        flo = pred.clone().cpu()  # to be used for warping ops\n",
    "        pred = pred.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "\n",
    "        # visualize flow\n",
    "        flow_vis = flowToColor(pred)\n",
    "        flow_vis_path = os.path.join(save_dir_vis, os.path.basename(image1_name))\n",
    "        cv2.imwrite(flow_vis_path, flow_vis[:, :, [2,1,0]])\n",
    "\n",
    "        # visualize forward warping\n",
    "        img = torch.from_numpy(image1).permute(2, 0, 1).unsqueeze(0)\n",
    "        image1_fwarp, _ = fwarp_wrapper(img=img, flo=flo)\n",
    "        fwarp_path = os.path.join(save_dir_fwarp, os.path.basename(image1_name))\n",
    "        cv2.imwrite(fwarp_path, image1_fwarp[:, :, [2,1,0]])\n",
    "\n",
    "        # grid visualization\n",
    "        row_1 = np.concatenate([image1, image2], axis=1)\n",
    "        row_2 = np.concatenate([flow_vis, image1_fwarp], axis=1)\n",
    "        grid_vis = np.concatenate([row_1, row_2], axis=0)\n",
    "        grid_vis_path = os.path.join(save_dir_grid, os.path.basename(image1_name))\n",
    "        cv2.imwrite(grid_vis_path, grid_vis[:, :, [2,1,0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "croco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "59e34348896a59af2bc1c8a469890c036aa50f08952549fa19204c9fd11494c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
